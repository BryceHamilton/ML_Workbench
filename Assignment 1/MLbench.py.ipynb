{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up Classification Models\n",
    "- gnb = Gaussian Naive Bayes\n",
    "- base_t = Default Decision tree\n",
    "- per = Perceptron\n",
    "- base_mlp = Default Multi-Layer Perceptron"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import Models\n",
    "import csv\n",
    "gnb = GaussianNB()\n",
    "base_t = tree.DecisionTreeClassifier()\n",
    "per = Perceptron()\n",
    "base_mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Imports\n",
    "### Imports the data from data 1 and data 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "test_data_1 = np.genfromtxt('./data 1/test_no_label_1.csv', delimiter=',')\n",
    "test_data_2 = np.genfromtxt('./data 2/test_no_label_2.csv', delimiter=',')\n",
    "\n",
    "labeled_test_data_1 = np.genfromtxt('./data 1/test_with_label_1.csv', delimiter=',')\n",
    "labeled_test_data_2 = np.genfromtxt('./data 2/test_with_label_2.csv', delimiter=',')\n",
    "\n",
    "train_data_1 = np.genfromtxt('./data 1/train_1.csv', delimiter=',')\n",
    "train_data_2 = np.genfromtxt('./data 2/train_2.csv', delimiter=',')\n",
    "\n",
    "val_data_1 = np.genfromtxt('./data 1/val_1.csv', delimiter=',')\n",
    "val_data_2 = np.genfromtxt('./data 2/val_2.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reference Dictionaries\n",
    "### Sets up dictionaries to translate values to symbolic names ({0 : 'A', 1 : 'B', etc}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Value Dictionaries\n",
    "data_1 = {}\n",
    "with open('data 1/info_1.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader, None) # Skip header\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for rows in reader:\n",
    "            data_1[int(rows[0])] = rows[1]\n",
    "data_2 = {}\n",
    "with open('data 2/info_2.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader, None) # Skip header\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for rows in reader:\n",
    "            data_2[int(rows[0])] = rows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Utility Method\n",
    "### Reusable method to return a scatter plot of frequency distributions for a given data set\n",
    "- takes a dataset as input, and the plot title\n",
    "    - gets labels for dataset by extracting last column of each row: data_labels\n",
    "    - generates a tuple (count, label) for each label found\n",
    "    - reduces list of tuples to a set to get one of each label only\n",
    "    - assigns labels to x axis on data_x and counts to y axis on data_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution Plot Methods\n",
    "def distribution(data, title, ref):\n",
    "    data_labels = [ref[(row[-1])] for row in data]\n",
    "    tuples = sorted(set([(label, data_labels.count(label)) for label in data_labels]))\n",
    "    print(tuples)\n",
    "    \n",
    "    data_x = [x[0] for x in tuples]\n",
    "    data_y = [y[1] for y in tuples]\n",
    "    \n",
    "    plt.scatter(data_x, data_y)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Frequencies\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distributions for Data 1 and Data 2\n",
    "- Training Set\n",
    "- Labeled Test Set\n",
    "- Validation Set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distributions for Data 1\n",
    "distribution(train_data_1, \"Data 1 Train Class / Freq\", data_1)\n",
    "distribution(labeled_test_data_1, \"Data 1 Test Class / Freq\", data_1)\n",
    "distribution(val_data_1, \"Data 1 Validation Class / Freq\", data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distributions for Data 2\n",
    "distribution(train_data_2, \"Data 2 Train Class / Freq\", data_2)\n",
    "distribution(labeled_test_data_2, \"Data 2 Test Class / Freq\", data_2)\n",
    "distribution(val_data_2, \"Data 2 Validation Class / Freq\", data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# General Model Testing/Training Procedure (Supervised)\n",
    "### Training\n",
    "- Split training data into features vectors (x_train) and label vector (y_train)\n",
    "- Fit data to the model\n",
    "\n",
    "### Testing\n",
    "- Split test data into features vector (x_test) and label vector (y_test)\n",
    "- Get label classification vector from trained model (prediction)\n",
    "\n",
    "### Output\n",
    "- Print actual labels\n",
    "- Print model label predictions\n",
    "- Print model label predictions to csv file\n",
    "- Print prediction misses (comparision between prediction and y_test)\n",
    "- Compare model label classification vector (prediction) to label vector (y_test)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GNB-DS1\n",
    "x_train = train_data_1[ :, :-1]\n",
    "y_train = train_data_1[ :, -1]\n",
    "\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "x_test = labeled_test_data_1[ :, :-1]\n",
    "y_test = labeled_test_data_1[ :, -1]\n",
    "\n",
    "prediction = gnb.predict(x_test)\n",
    "print([(entry, data_1[prediction[entry]]) for entry in range(prediction.size)])\n",
    "print([(entry, data_1[y_test[entry]]) for entry in range(y_test.size)])\n",
    "print((y_test != prediction).sum())\n",
    "with open('GNB-DS1.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for entry in range(prediction.size):\n",
    "        writer.writerow([entry, int(prediction[entry])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GNB-DS2\n",
    "x_train = train_data_2[ :, :-1]\n",
    "y_train = train_data_2[ :, -1]\n",
    "\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "x_test = labeled_test_data_2[ :, :-1]\n",
    "y_test = labeled_test_data_2[ :, -1]\n",
    "\n",
    "prediction = gnb.predict(x_test)\n",
    "print([(entry, data_2[prediction[entry]]) for entry in range(prediction.size)])\n",
    "print([(entry, data_2[y_test[entry]]) for entry in range(y_test.size)])\n",
    "print((y_test != prediction).sum())\n",
    "with open('GNB-DS2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for entry in range(prediction.size):\n",
    "        writer.writerow([entry, int(prediction[entry])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Base-DT-DS1\n",
    "x_train = train_data_1[ :, :-1]\n",
    "y_train = train_data_1[ :, -1]\n",
    "\n",
    "base_t.fit(x_train, y_train)\n",
    "\n",
    "x_test = labeled_test_data_1[ :, :-1]\n",
    "y_test = labeled_test_data_1[ :, -1]\n",
    "\n",
    "prediction = base_t.predict(x_test)\n",
    "print([(entry, data_1[prediction[entry]]) for entry in range(prediction.size)])\n",
    "print([(entry, data_1[y_test[entry]]) for entry in range(y_test.size)])\n",
    "print((y_test != prediction).sum())\n",
    "\n",
    "with open('Base-DT-DS1.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for entry in range(prediction.size):\n",
    "        writer.writerow([entry, int(prediction[entry])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Base-DT-DS2\n",
    "x_train = train_data_2[ :, :-1]\n",
    "y_train = train_data_2[ :, -1]\n",
    "\n",
    "base_t.fit(x_train, y_train)\n",
    "\n",
    "x_test = labeled_test_data_2[ :, :-1]\n",
    "y_test = labeled_test_data_2[ :, -1]\n",
    "\n",
    "prediction = base_t.predict(x_test)\n",
    "print([(entry, data_2[prediction[entry]]) for entry in range(prediction.size)])\n",
    "print([(entry, data_2[y_test[entry]]) for entry in range(y_test.size)])\n",
    "prediction = base_t.predict(x_test)\n",
    "print((y_test != prediction).sum())\n",
    "\n",
    "with open('Base-DT-DS2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for entry in range(prediction.size):\n",
    "        writer.writerow([entry, int(prediction[entry])])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Brute Force Tree Optimization\n",
    "- Trains new tree with different h-parameter to determine best performance\n",
    "    - Loops through a variety of parameters, tracking the best case of predictions misses (best)\n",
    "- Findings\n",
    "    - Entropy as a split criterion is the best h-parameter\n",
    "    - No max depth for the tree is the best h-parameter\n",
    "    - Balanced class weights is the best h-parameter\n",
    "    - Min Samples Split works best at very small numbers\n",
    "        - testing with range down to 1/100000\n",
    "    - Min Impurity Decrease works best at very small numbers\n",
    "        - testing with range down to 1/100000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tree Optimization DS1\n",
    "x_train = train_data_1[ :, :-1]\n",
    "y_train = train_data_1[ :, -1]\n",
    "\n",
    "x_test = labeled_test_data_1[ :, :-1]\n",
    "y_test = labeled_test_data_1[ :, -1]\n",
    "\n",
    "best = 80\n",
    "\n",
    "def tree_score(sample, impurity):\n",
    "    best_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", # Entropy is the best split criterion\n",
    "                                       max_depth=None, # No max depth is best h-parameter\n",
    "                                       min_samples_split=sample, # Small numbers here for best predictions\n",
    "                                       min_impurity_decrease=impurity, # Small number here for best predictions\n",
    "                                       class_weight=\"balanced\") # Balanced class weights is the best h-paramter\n",
    "    best_tree.fit(x_train, y_train)\n",
    "    pred = best_tree.predict(x_test)\n",
    "    return (y_test != pred).sum()\n",
    "\n",
    "for samples in [1/x for x in range(1000, 1000000, 10000)]: # Min Number of Samples\n",
    "    for impurities in [1/y for y in range(1000, 1000000, 10000)]: # Min Impurity Decrease\n",
    "            missed = tree_score(samples, impurities)\n",
    "            if missed < best:\n",
    "                print(f'Splitting Criterion: Entropy\\n'\n",
    "                      f'Max Depth: None\\n'\n",
    "                      f'Min # Samples: {samples}\\n'\n",
    "                      f'Min Impurity Decrease:{impurities}\\n'\n",
    "                      f'Class Weights: Balanced')\n",
    "                best = missed\n",
    "                print(missed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}